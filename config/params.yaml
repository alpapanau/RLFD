# Configuration for the RLFD (Reinforcement Learning Fraud Detection) Project

# --- Data and Preprocessing ---
data:
  path: "/eos/home-i01/a/alpapana/SWAN_projects/Bancari/Data/bonifico/" # UPDATE THIS PATH
  results_log_file: "results_log.csv"
  model_save_path: "models/best_model.pth" # Path to save the best performing model
  fine_tuned_model_save_path: "models/fine_tuned"
preprocessing:
  window_size: 18
  top_n_categorical: 10 # Number of categorical features to keep based on mutual information
  top_m_values: 10      # Number of frequent values to keep per categorical feature
  test_split_size: 0.2
  validation_split_size: 0.2 # Proportion of the training+validation set for validation

# --- Model Architecture ---
model:
  type: "lstm" # or "transformer"
  hidden_size: 64
  # Transformer-specific params (if model.type is "transformer")
  n_heads: 4
  num_layers: 1
  dropout: 0.1

# --- Training Parameters ---
training:
  seed: 10022 # Set to null for a random seed in each run
  num_episodes: 800
  inner_epochs: 200
  batch_size: 8
  learning_rate: 0.001
  gamma: 0.95 # Discount factor for future rewards
  epsilon_min: 0.22 # Minimum exploration rate
  target_update_freq: 40
  replay_buffer_size: 80
  # Rewards
  r1_positive_fraud: 4.0   # Reward for correctly identifying fraud
  r2_positive_normal: 1.0  # Reward for correctly identifying normal
  validation_freq: 10      # Run validation every N episodes

# --- Targeted Fine-Tuning Phase (Optional) ---

fine_tuning:
  enabled: true # Set to true to run the fine-tuning phase after main training
  k_percent_uncertain: .1 # Fine-tune on the 10% most uncertain samples
  epochs: 100 # Number of fine-tuning epochs (keep this low)
  learning_rate: 0.0001 # Use a very small learning rate